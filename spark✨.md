# Machine-learning-with-apache-spark‚ú®

## Getting started 

**Importing Libraries**
```py
# FindSpark simplifies the process of using Apache Spark with Python

import findspark
findspark.init()

# import SparkSession
from pyspark.sql import SparkSession
```

Notes:
- SparkContext ‚Äî —ç—Ç–æ "—Å—Ç–∞—Ä—ã–π —Å–ø–æ—Å–æ–±" –Ω–∞—á–∞—Ç—å —Ä–∞–±–æ—Ç—É —Å–æ Spark, –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ –∫–ª–∞—Å—Ç–µ—Ä—É –∏ —É–ø—Ä–∞–≤–ª—è—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω–∏—è–º–∏, –Ω–æ —Å–∞–º –ø–æ —Å–µ–±–µ –Ω–µ –≤–∫–ª—é—á–∞–µ—Ç —Ä–∞–±–æ—Ç—É —Å DataFrame –∏ SQL.
- SparkSession ‚Äî —ç—Ç–æ "–Ω–æ–≤—ã–π –∏ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Å–ø–æ—Å–æ–±", –ø–æ—è–≤–∏–≤—à–∏–π—Å—è –Ω–∞—á–∏–Ω–∞—è —Å Spark 2.0, –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤ —Å–µ–±–µ SparkContext, SQLContext –∏ HiveContext. –¢–æ –µ—Å—Ç—å –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Å RDD (–∫–∞–∫ SparkContext), –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Å DataFrame, SQL, Hive, Parquet –∏ —Ç.–ø.
- findspark –Ω—É–∂–µ–Ω, –µ—Å–ª–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è SPARK_HOME –∏ PYTHONPATH –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã. –ù–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ –≤ Jupyter Notebook –ª–æ–∫–∞–ª—å–Ω–æ, –∫–æ–≥–¥–∞ PySpark –Ω–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏. –ù–µ –Ω—É–∂–µ–Ω –≤ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω–æ–π —Å—Ä–µ–¥–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, Databricks, EMR, Docker, –∏–ª–∏ –≤–Ω—É—Ç—Ä–∏ PySpark-–∫–ª–∞—Å—Ç–µ—Ä–∞) –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è PySpark –∏–∑ pip (pip install pyspark).


```py
# create spark session
spark = SparkSession.builder.appName("Diamond data analysis").getOrCreate()

# load data
# !wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/datasets/diamonds.csv

# data to a df
diamond_data = spark.read.csv("diamonds.csv", header=True, inferSchema=True)

# explore data
diamond_data.printSchema()
diamond_data.head(5)
diamond_data.show(5)

# stop the spark session
spark.stop()
```

## Regression


| Built-in Model                            |`pyspark.ml.regression` | 
| -------------------------------- | ------------------------------- | 
| üí° Linear Regression             | `LinearRegression`              | 
| üå≤ Decision Tree Regressor       | `DecisionTreeRegressor`         | 
| üå≥ Random Forest Regressor       | `RandomForestRegressor`         | 
| üéØ GBT Regressor (Boosting)      | `GBTRegressor`                  | 
| ü§ñ AFT Survival Regression       | `AFTSurvivalRegression`         | 
| üßÆ Generalized Linear Regression | `GeneralizedLinearRegression`   |

Notes:
- Spark ML —Ç—Ä–µ–±—É–µ—Ç, —á—Ç–æ–±—ã –≤—Å–µ –≤—Ö–æ–¥–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –±—ã–ª–∏ –≤ –æ–¥–Ω–æ–º —Å—Ç–æ–ª–±—Ü–µ —Ç–∏–ø–∞ Vector, –∞ –Ω–µ –≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–∫–∞—Ö

| üìè Metric       | `.setMetricName()` value | Description                                                                     |
| --------------- | ------------------------ | ------------------------------------------------------------------------------- |
| üßÆ **RMSE**     | `"rmse"`                 | Root Mean Squared Error ‚Äì penalizes large errors more heavily.                  |
| ‚ûï **MSE**       | `"mse"`                  | Mean Squared Error ‚Äì average squared difference between predicted and actual.   |
| ‚ûñ **MAE**       | `"mae"`                  | Mean Absolute Error ‚Äì average absolute difference between predicted and actual. |
| üìà **R¬≤ Score** | `"r2"`                   | Coefficient of Determination ‚Äì proportion of variance explained by the model.   |


### Linear regression

Let's use `diamond_data` from the previous section:

```py

from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import LinearRegression, GBTRegressor
from pyspark.ml.evaluation import RegressionEvaluator

# Prepare feature vector
assembler = VectorAssembler(inputCols=["carat", "depth", "table"], outputCol="features")
diamond_transformed_data = assembler.transform(diamond_data)

# Print the vectorized features and label columns
diamond_transformed_data.select("features","price").show()

# Train-test split
(training_data, testing_data) = diamond_transformed_data.randomSplit([0.7, 0.3], seed=42)

# Train LR model
lr = LinearRegression(featuresCol="features", labelCol="price")
model = lr.fit(training_data)

# Make prediction
predictions = model.transform(testing_data)

# Print metrics
metrics = {}
for metric in ['r2', 'mae', 'rmse']:
    evaluator = RegressionEvaluator(labelCol="price", predictionCol="prediction", metricName=metric)
    metrics[metric] = evaluator.evaluate(predictions)
print(metrics)

```

### GBT
```oy
from pyspark.ml.regression import GBTRegressor

gbt = GBTRegressor(featuresCol="features", labelCol="price")
model = gbt.fit(training_data)
predictions = model.transform(testing_data)

metrics = {}
for metric in ['r2', 'mae', 'rmse']:
    evaluator = RegressionEvaluator(labelCol="price", predictionCol="prediction", metricName=metric)
    metrics[metric] = evaluator.evaluate(predictions)
print(metrics)

```


### LightGBM

*Works in theory

```py
!pip install synapseml

from pyspark.sql import SparkSession

from pyspark.ml.feature import VectorAssembler
from pyspark.ml.evaluation import RegressionEvaluator

from synapse.ml.lightgbm import LightGBMRegressor

spark = SparkSession.builder \
    .appName("LightGBM with SynapseML") \
    .config("spark.jars.packages", "com.microsoft.azure:synapseml_2.12:0.11.2") \
    .getOrCreate()

# MOAR RESOURCES, MOAAAR
#spark = SparkSession.builder \
#    .appName("SynapseML Example") \
#    .config("spark.jars.packages", "com.microsoft.azure:synapseml_2.12:0.11.2") \
#    .config("spark.executor.memory", "2g") \
#    .config("spark.driver.memory", "2g") \
#    .getOrCreate()

!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/datasets/diamonds.csv
diamond_data = spark.read.csv("diamonds.csv", header=True, inferSchema=True)

assembler = VectorAssembler(inputCols=["carat", "depth", "table"], outputCol="features")
diamond_transformed_data = assembler.transform(diamond_data)
(training_data, testing_data) = diamond_transformed_data.randomSplit([0.7, 0.3], seed=42)

lgbm = LightGBMRegressor(
    featuresCol="features",
    labelCol="price",
    # numIterations=100, learningRate=0.1, numLeaves=31
)

model = lgbm.fit(train_data)
predictions = model.transform(testing_data)
metrics = {}
for metric in ['r2', 'mae', 'rmse']:
    evaluator = RegressionEvaluator(labelCol="price", predictionCol="prediction", metricName=metric)
    metrics[metric] = evaluator.evaluate(predictions)
print(metrics)
```

## Classification

| Model                      | `pyspark.ml.classification`              | Task Type          | Notes                                            |
| ----------------------------- | -------------------------------- | --------------------- | --------------------------------------------------- |
| üìà Logistic Regression   | `LogisticRegression`             | Binary / Multiclass   | Supports regularization, baseline model             |
| ‚öñÔ∏è Linear SVM             | `LinearSVC`                      | Binary only           | No probability outputs, fast and scalable           |
| üìä Naive Bayes            | `NaiveBayes`                     | Binary / Multiclass   | Good for categorical/text data                      |
| üå≥ Decision Tree          | `DecisionTreeClassifier`         | Binary / Multiclass   | Interpretable, prone to overfitting                 |
| üå≤ Random Forest          | `RandomForestClassifier`         | Binary / Multiclass   | Robust, good default ensemble                       |
| üî• Gradient-Boosted Trees | `GBTClassifier`                  | Binary only           | Powerful, but no multiclass support                 |
| üß© One-vs-Rest            | `OneVsRest`                      | Multiclass via binary | Wraps binary classifiers for multiclass             |
| üß† Multilayer Perceptron  | `MultilayerPerceptronClassifier` | Binary / Multiclass   | Simple feedforward neural net                       |
| üßÆ Factorization Machines | `FMClassifier`                   | Binary only           | Great for sparse feature data (e.g. CTR prediction) |

### MulticlassClassificationEvaluator

| Metric        | `.setMetricName()` value | Description                                            |
| ---------------- | ------------------------ | ------------------------------------------------------ |
| üßÆ **Accuracy**  | `"accuracy"`             | Proportion of correct predictions (macro average).     |
| üìê **F1 Score**  | `"f1"`                   | Harmonic mean of precision and recall (macro average). |
| üéØ **Precision** | `"weightedPrecision"`    | Precision weighted by class frequencies.               |
| üéØ **Recall**    | `"weightedRecall"`       | Recall weighted by class frequencies.                  |

## Metrics 

| üì¶ Evaluator Class                     | üß† Task Type                 | üìè Supported Metrics (`.setMetricName()`)                       | üîé Description                                                          |
| -------------------------------------- | ---------------------------- | --------------------------------------------------------------- | ----------------------------------------------------------------------- |
| üßÆ `MulticlassClassificationEvaluator` | Classification (multi-class) | `"accuracy"`, `"f1"`, `"weightedPrecision"`, `"weightedRecall"` | Evaluates multi-class classification models.                            |
| ‚úÖ `BinaryClassificationEvaluator`      | Classification (binary)      | `"areaUnderROC"`, `"areaUnderPR"`                               | Evaluates binary classifiers based on ROC or PR curves.                 |
| üìâ `RegressionEvaluator`               | Regression                   | `"rmse"`, `"mse"`, `"mae"`, `"r2"`                              | Evaluates regression models.                                            |
| üìä `ClusteringEvaluator`               | Clustering                   | `"silhouette"`                                                  | Measures clustering quality using silhouette score (based on distance). |

## Unsupervised learning

| üî¢ Model                           | üß± PySpark Class                    | üß† Task Type             | üìù Notes                                                      |
| ---------------------------------- | ----------------------------------- | ------------------------ | ------------------------------------------------------------- |
| üìä **K-Means**                     | `KMeans`                            | Clustering               | Fast, popular centroid-based clustering.                      |
| üß≠ **Gaussian Mixture**            | `GaussianMixture`                   | Clustering               | Soft clustering (probabilistic assignment).                   |
| üî≥ **Bisecting K-Means**           | `BisectingKMeans`                   | Clustering               | Hierarchical divisive clustering.                             |
| üßÆ **LDA**                         | `LDA` (Latent Dirichlet Allocation) | Topic Modeling           | Used for discovering abstract topics in text.                 |
| üßä **PCA**                         | `PCA`                               | Dimensionality Reduction | Projects features onto principal components.                  |
| üß¨ **Truncated SVD**               | `TruncatedSVD` *(SynapseML)*        | Dimensionality Reduction | Similar to PCA but works on sparse data (requires SynapseML). |
| üß± **Normalizer / StandardScaler** | `Normalizer`, `StandardScaler`      | Preprocessing            | Not models but often used before unsupervised learning.       |

## Dataframes

```py
# print the number of rows in the dataframe
df.count()

# Drop Duplicates
df = df.dropDuplicates()

# Drop Null values
df=df.dropna()

# Write the data to a Parquet fileSSSSS
df.write.mode("overwrite").parquet("student-hw.parquet")

# Reduce the number of partitions in the dataframe to one - and then save to a parquet
df = df.repartition(1)

# Read parquet
df = spark.read.parquet("student-hw-single.parquet")
```
Notes: To improve parallellism, spark stores each dataframe in multiple partitions.
When the data is saved as parquet file, each partition is saved as a separate file!!!

For data transformations: 
```
#import the expr function that helps in transforming the data
from pyspark.sql.functions import expr

# Convert pounds to kilograms
# Multiply weight_pounds with 0.453592 to get a new column weight_kg
df = df.withColumn("weight_kg", expr("weight_pounds * 0.453592"))
```
